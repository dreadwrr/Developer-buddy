#!/bin/bash
# 08/11/2025
asz() {
awk 'NF && $8 ~ /^[0-9]+$/ && $8 > 0 {
    filesize = $8
    total_size += filesize
    file_count++
}
END {
    if (file_count > 0) {
        avg_size_kb = total_size / file_count / 1024
        printf "Average file size: %.2f KB\n", avg_size_kb
    } else {
        print "No file sizes."
    }
}' "$1"
}
aat () {
awk '
{
    # Match the timestamp fields (access date and time)
    match($0, /"([^"]+)"/, arr)
    rest = substr($0, RSTART + RLENGTH)
    n = split(rest, fields, " ")

    access_date = fields[2]
    access_time = fields[3]

    # If the date and time format matches, process it
    if (access_date ~ /^[0-9]{4}-[0-9]{2}-[0-9]{2}$/ && access_time ~ /^[0-9]{2}:[0-9]{2}:[0-9]{2}$/) {
        datetime = access_date " " access_time
        cmd = "date -d \"" datetime "\" +%s"
        cmd | getline epoch
        close(cmd)

        if (epoch != "") {
            # Store the first access time as the min and max
            if (count == 0) {
                first_epoch = epoch
                last_epoch = epoch
            } else {
                # Find the min and max epoch
                if (epoch < first_epoch) first_epoch = epoch
                if (epoch > last_epoch) last_epoch = epoch
            }

            sum += epoch
            count++
        }
    }
}

END {
    # Output the breakdown
    print "Search breakdown"
    printf "\033[36mAverage access time:\033[0m\n"

    # Print the range of access times (first and last access)
    if (count > 0) {
        # Comment out or delete the lines that print the earliest and latest access times
        # print "Earliest access time (epoch): " first_epoch
        # print "Latest access time (epoch): " last_epoch

        # Now, filter the access times that are within the range
        valid_count = 0
        valid_sum = 0

        # Re-process each line and only count those within the range
        for (i = 1; i <= count; i++) {
            if (epoch >= first_epoch && epoch <= last_epoch) {
                valid_sum += epoch
                valid_count++
            }
        }

        # If we have valid times, calculate the average
        if (valid_count > 0) {
            avg = int(valid_sum / valid_count)
            cmd = "date -d @" avg " \"+%Y-%m-%d %H:%M:%S\""
            cmd | getline avg_date
            close(cmd)

            print "Filtered Average Date and Time: " avg_date
        } else {
            print "No valid access times found within range."
        }

    } else {
        print "No valid access times found."
    }
}
' "$1"
echo
}
#most common file extension or if none then output none.
ext() {
#awk '{print $3}' logfile.txt | grep -oE '\.[^./ ]+$' | sort | uniq -c | sort -nr | head -3      only extensions
awk '{
    # Extract quoted file path using regex
    match($0, /"[^"]+"/, path_match)
    path = path_match[0]
    gsub(/"/, "", path)

    # Extract filename from path
    n = split(path, parts, "/")
    fname = parts[n]

    # Extract extension
    if (fname ~ /\.[^./]+$/) {
        ext = substr(fname, index(fname, "."))
    } else {
        ext = "[no extension]"
    }

    print ext
}' $1 | sort | uniq -c | sort -nr | head -3

echo
}
#Avg hour top or bottom of hour? time? of entire logs or statpst
hr() {
	awk '
	{
		date = $1
		time = $2

		# Skip lines without valid time
		if (length(time) < 5) next

		hour = substr(time, 1, 2)
		minute = substr(time, 4, 2)

		# Validate hour and minute
		if (hour ~ /^[0-9]{2}$/ && minute ~ /^[0-9]{2}$/) {
			count[date, hour]++
			days[date] = 1
			minutes_arr[date, hour, minute]++
		}
	}

	END {
		day_count = length(days)
		if (day_count == 0) {
			print "No data to process."
			exit 1
		}

		# Sum counts per hour across all days
		for (key in count) {
			split(key, parts, SUBSEP)
			hour = parts[2]
			hour_totals[hour] += count[key]
		}

		max_avg = -1
		max_hour = -1

		# Find hour with highest average activity
		for (h = 0; h < 24; h++) {
			hh = sprintf("%02d", h)
			avg = (hour_totals[hh] ? hour_totals[hh] / day_count : 0)
			if (avg > max_avg) {
				max_avg = avg
				max_hour = h
			}
		}

		max_hour_str = sprintf("%02d", max_hour)

		# Calculate weighted average minute within peak hour
		sum_minutes = 0
		total_counts = 0
		for (key in minutes_arr) {
			split(key, parts, SUBSEP)
			d = parts[1]
			h = parts[2]
			m = parts[3] + 0
			if (h == max_hour_str) {
				c = minutes_arr[key]
				sum_minutes += m * c
				total_counts += c
			}
		}

		avg_minute = (total_counts > 0) ? sum_minutes / total_counts : 0
		avg_min_int = int(avg_minute)

		# Decide top or bottom half
		half = (avg_minute < 30) ? "top of the hour" : "bottom of the hour"

		printf "Avg time of file activity: %02d:%02d (%.2f minutes)\n", max_hour, avg_min_int, avg_minute
		printf "(%s)\n", half
	}
	' "$1"
}

{
echo "Top 5 files created (awk generated)"
awk '
{
    # Split line by comma
    n = split($0, fields, ",")

    # Join fields from 3rd to end in case filepath contains commas
    filepath = fields[3]
    for (i = 4; i <= n; i++) filepath = filepath "," fields[i]

    # Trim leading/trailing spaces
    gsub(/^ +| +$/, "", filepath)

    print filepath
}' "$1" | sort | uniq -c | sort -nr | head -n 5

# Session type
#
echo "Type" #Heuristic session?
awk -v RS="" -v limit=15 '
# Function to convert timestamp to epoch
function to_epoch(ts,    cmd, epoch) {
  cmd = "date -d \"" ts "\" +%s"
  cmd | getline epoch
  close(cmd)
  return epoch
}

# First pass: collect all sessions
{
  sessions[NR] = $0
}

END {
  session_limit = 15
  start = (NR > session_limit) ? NR - session_limit + 1 : 1

  for (i = start; i <= NR; i++) {
    process_session(sessions[i], i - start + 1)
  }
}

# Function to process a session and print classification
function process_session(block, snum,    lines, n, i, parts, timestamp, times, gaps_sum, gaps_count, max_gap, gap, avg_gap, threshold, formatted_timestamp) {
  n = split(block, lines, "\n")
  delete times

  # Extract timestamp from the first line and store it in `formatted_timestamp`
  split(lines[1], parts, " ")aat $ofile
  formatted_timestamp = parts[1] " " parts[2]

  # Calculate the epoch times for each line
  for (i = 1; i <= n; i++) {
    split(lines[i], parts, " ")
    timestamp = parts[1] " " parts[2]
    times[i] = to_epoch(timestamp)
  }

  gaps_sum = 0
  gaps_count = 0
  max_gap = 0

  # Calculate gaps between consecutive entries
  for (i = 2; i <= n; i++) {
    gap = times[i] - times[i-1]
    gaps_sum += gap
    gaps_count++
    if (gap > max_gap) max_gap = gap
  }

  avg_gap = (gaps_count > 0) ? gaps_sum / gaps_count : 0
  threshold = 60

  # Output the session with date and time of the first log entry
  if (max_gap > threshold) {
    print "Snapshot " snum ": " formatted_timestamp " - Separate session (max gap " max_gap "s > " threshold "s)"
  } else {
    print "Snapshot " snum ": " formatted_timestamp " - Burst session (max gap " max_gap "s â‰¤ " threshold "s)"
  }
}
' "$1"
echo
echo "Most Replaced by inode (awk generated)"
awk -v RS="" -v max=7 '
function process_snapshot(snapshot,   i, lines, path, inode, rest, ino_match, n, fields, filepath) {
  n = split(snapshot, lines, "\n")
  for (i = 1; i <= n; i++) {
    # Split the line by comma
    split(lines[i], fields, ",")

    # Join fields from the 3rd to the end (in case filepath contains commas)
    filepath = fields[3]
    for (j = 4; j <= length(fields); j++) {
      filepath = filepath "," fields[j]
    }

    # Trim leading/trailing spaces from filepath
    gsub(/^ +| +$/, "", filepath)

    # Get inode number after filepath (assuming inode is the first number after the filepath)
    rest = substr(lines[i], index(lines[i], filepath) + length(filepath))
    if (match(rest, /[0-9]+/, ino_match)) {
      inode = ino_match[0]
    } else {
      inode = ""
    }

    if (filepath == "" || inode == "") continue

    if (prev_inode[filepath] && prev_inode[filepath] != inode) {
      changes[filepath]++
      found = 1        # Mark that we found at least one change
    }
    prev_inode[filepath] = inode
  }
}
}

{
  snapshots[NR] = $0
}

END {
  start = (NR > max) ? NR - max + 1 : 1
  for (i = start; i <= NR; i++) {
    process_snapshot(snapshots[i])
  }
  if (found) {
    for (p in changes) print changes[p], p
  } else {
    print "No results"
  }
}
' "$1" | sort -rn | head -7

} >> $2 2>&1
